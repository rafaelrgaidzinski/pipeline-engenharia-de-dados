# Pipeline de Engenharia de Dados com Airflow + PostgreSQL

ğŸ“Œ Objetivo

Este projeto tem como objetivo construir um pipeline de dados que realiza:
1. Leitura de arquivos .csv
2. Carga em um banco de dados PostgreSQL
3. OrquestraÃ§Ã£o da ingestÃ£o via Airflow
4. Base para um modelo dimensional que serÃ¡ consumido por dashboards


ğŸ—ï¸ Estrutura do 

pipeline-engenharia-de-dados/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ orders.csv
â”‚   â””â”€â”€ ...demais CSVs
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ populate_postgres.py
â”œâ”€â”€ .env
â””â”€â”€ README.


âš™ï¸ PrÃ©-requisitos

1. Docker e Docker Compose instalados
2. Python 3.12 (caso deseje rodar os scripts fora do Airflow)
3. psycopg2, pandas, sqlalchemy, python-dotenv

ğŸ˜ Subindo PostgreSQL e Airflow

1. CriaÃ§Ã£o do container PostgreSQL (caso queira isolado):
docker run -d \
  --name postgres \
  -e POSTGRES_USER=airflow \
  -e POSTGRES_PASSWORD=airflow \
  -e POSTGRES_DB=airflow \
  -p 5432:5432 \
  postgres:
  
2. Subindo o ambiente Airflow com PostgreSQL:
2.1. Dentro da pasta airflow/, execute:
export AIRFLOW_UID=$(id -u)
docker compose up airflow-init
docker compose up -d

2.2. O Airflow serÃ¡ exposto em: http://localhost:8080
Credenciais padrÃ£o:
UsuÃ¡rio: admin
Senha: admin

ğŸ“¥ Populando o PostgreSQL com os arquivos CSV

Execute o script scripts/popular_postgre.py:
python scripts/populate_postgres.py

Esse script: 
- LÃª os arquivos .csv da pasta data/
- Cria automaticamente as tabelas no banco PostgreSQL
- Insere os dados via SQLAlchemy

ğŸ”„ Acessando o banco PostgreSQL no terminal (WSL ou Linux)

docker exec -it postgres psql -U airflow -d airflow